# Story 3.8 - Scan & D√©duplication PC-wide

**Epic** : 3 - Archiviste & Recherche Documentaire
**Estimation** : M (12-18h)
**Status** : backlog
**D√©pendances** : Story 3.5 (Surveillance dossiers Photos/Documents)

---

## üìã Objectif

Scanner **tous les fichiers** du PC Mainteneur (`C:\Users\lopez\`) pour d√©tecter et supprimer intelligemment les doublons via SHA256, avec chemins prioritaires et r√®gles de s√©lection.

---

## üéØ User Story

**En tant que** Mainteneur,
**Je veux** scanner l'int√©gralit√© de mon PC pour d√©tecter les doublons (photos, documents, vid√©os, etc.),
**Afin de** lib√©rer de l'espace disque en gardant automatiquement la meilleure copie selon des r√®gles de priorit√© claires.

---

## ‚úÖ Acceptance Criteria

### AC1 - Scan PC-wide avec exclusions syst√®me

```python
# Chemin racine
SCAN_ROOT = r"C:\Users\lopez\"

# Exclusions (syst√®me Windows + cache)
EXCLUDED_FOLDERS = [
    "AppData",
    "Application Data",
    ".cache",
    ".vscode",
    ".claude",
    "node_modules",
    "__pycache__",
]
```

- **Scan r√©cursif** de tous sous-dossiers (hors exclusions)
- **Types support√©s** : photos (jpg, png, heic, raw), documents (pdf, docx, xlsx), vid√©os (mp4, mov, avi), tous autres fichiers
- **Logging** : progression (X/Y fichiers scann√©s, X Go trait√©s)

### AC2 - Calcul SHA256 universel

- **Hash SHA256** calcul√© pour chaque fichier (taille > 0 octet)
- **Stockage** : `C:\Friday\scan-cache\sha256.db` (SQLite local pour perf)
- **Format table** :
  ```sql
  CREATE TABLE file_hashes (
      sha256 TEXT PRIMARY KEY,
      file_path TEXT NOT NULL,
      size_bytes INTEGER,
      resolution TEXT,      -- Pour photos (ex: "4032x3024")
      exif_date TEXT,       -- Pour photos (YYYY-MM-DD HH:MM:SS)
      created_at TIMESTAMP,
      UNIQUE(file_path)
  );
  ```
- **Incremental** : si fichier d√©j√† scann√© (mtime identique) ‚Üí skip recalcul

### AC3 - D√©tection doublons avec groupes

- **Groupes de doublons** : regrouper par SHA256
- **Filtrer** : garder seulement groupes avec ‚â•2 fichiers
- **Logging** : `X groupes de doublons d√©tect√©s, Y Go r√©cup√©rables`

### AC4 - R√®gles de priorit√© intelligentes

**Ordre prioritaire pour garder LE fichier** :

1. **Emplacement** (ordre d√©croissant) :
   - `C:\Users\lopez\BeeStation\Photos\` (priorit√© absolue photos)
   - `C:\Users\lopez\BeeStation\Documents\` (priorit√© absolue documents)
   - Tous autres emplacements (√©galit√©)

2. **R√©solution** (photos uniquement) :
   - Plus haute r√©solution gagne (ex: 4032x3024 > 1920x1080)
   - Si non-photo ou r√©solution identique ‚Üí crit√®re suivant

3. **Date EXIF** (photos uniquement) :
   - Date EXIF la plus ancienne gagne (= original)
   - Si non-photo ou pas de date EXIF ‚Üí crit√®re suivant

4. **Nom de fichier** :
   - Nom le plus court gagne (ex: `IMG_1234.jpg` > `IMG_1234 (copie 2).jpg`)
   - Si √©galit√© parfaite ‚Üí garder le premier alphab√©tiquement

**Impl√©mentation** :
```python
def select_file_to_keep(duplicate_group: list[FileHash]) -> FileHash:
    """
    Retourne le fichier √† GARDER selon r√®gles de priorit√©.
    Les autres fichiers du groupe seront supprim√©s.
    """
    # 1. Trier par emplacement prioritaire
    # 2. Si √©galit√© ‚Üí trier par r√©solution (desc)
    # 3. Si √©galit√© ‚Üí trier par date EXIF (asc)
    # 4. Si √©galit√© ‚Üí trier par longueur nom (asc)
    # 5. Si √©galit√© ‚Üí trier par nom (alpha)
    return sorted(duplicate_group, key=priority_key)[0]
```

### AC5 - Mode Dry-Run obligatoire avec rapport CSV

**Workflow** :
1. Scan complet ‚Üí d√©tection doublons ‚Üí **DRY-RUN** (aucune suppression)
2. G√©n√©ration rapport CSV d√©taill√©
3. Envoi CSV via Telegram (document)
4. User valide ‚Üí ex√©cution r√©elle
5. Logging suppressions effectives

**Format rapport CSV** :
```csv
sha256,action,file_path,size_mb,resolution,exif_date,reason
abc123...,KEEP,C:\Users\lopez\BeeStation\Photos\Paris\IMG_1234.jpg,2.5,4032x3024,2024-01-15 14:30:00,Emplacement prioritaire
abc123...,DELETE,C:\Users\lopez\OneDrive\Photos\IMG_1234.jpg,2.5,4032x3024,2024-01-15 14:30:00,Doublon (emplacement inf√©rieur)
abc123...,DELETE,C:\Users\lopez\Downloads\IMG_1234 (2).jpg,2.5,4032x3024,2024-01-15 14:30:00,Doublon (nom plus long)
```

**Colonnes** :
- `sha256` : Hash du groupe
- `action` : KEEP | DELETE
- `file_path` : Chemin complet
- `size_mb` : Taille en Mo (2 d√©cimales)
- `resolution` : Pour photos (ex: "4032x3024")
- `exif_date` : Pour photos (ex: "2024-01-15 14:30:00")
- `reason` : Explication d√©cision (fran√ßais)

### AC6 - Commande Telegram `/scan-photos-pc`

```
User: /scan-photos-pc

Friday:
üîç Scan PC d√©marr√©
üìÅ Racine : C:\Users\lopez\
‚è±Ô∏è Estimation : 15-30 min pour 100+ Go

[30 min plus tard]

Friday:
‚úÖ Scan termin√©
üìä R√©sultat :
- 45 230 fichiers scann√©s (127 Go)
- 18 groupes de doublons d√©tect√©s
- üóëÔ∏è 32.4 Go r√©cup√©rables (453 fichiers √† supprimer)

üìÑ Rapport CSV joint (scan-doublons-2026-02-11.csv)

Commandes :
/exec-dedup - Ex√©cuter suppressions
/cancel-dedup - Annuler
```

**S√©curit√©s** :
- Timeout user : 7 jours (apr√®s ‚Üí annulation auto)
- `/exec-dedup` demande **confirmation finale** avec inline buttons [Confirmer] [Annuler]
- Logging complet : fichiers supprim√©s ‚Üí `C:\Friday\logs\dedup-2026-02-11.log`

### AC7 - Trust Layer & Action Receipt

```python
@friday_action(module="archiviste", action="dedup_scan", trust_default="auto")
async def scan_pc_for_duplicates() -> ActionResult:
    """
    Scan PC-wide, dry-run automatique.
    La suppression r√©elle n√©cessite validation user (trust=propose).
    """
    # Scan + SHA256 + d√©tection doublons
    report = await execute_scan()

    return ActionResult(
        input_summary="Scan PC complet (C:\\Users\\lopez\\)",
        output_summary=f"‚Üí {report.duplicate_groups} groupes, {report.recoverable_gb:.1f} Go r√©cup√©rables",
        confidence=1.0,  # Scan d√©terministe
        reasoning=f"SHA256 sur {report.total_files} fichiers, {len(EXCLUDED_FOLDERS)} dossiers exclus",
        payload={
            "total_files": report.total_files,
            "total_gb": report.total_gb,
            "duplicate_groups": report.duplicate_groups,
            "recoverable_gb": report.recoverable_gb,
            "csv_path": report.csv_path,
        }
    )

@friday_action(module="archiviste", action="dedup_execute", trust_default="propose")
async def execute_deduplication(csv_path: str) -> ActionResult:
    """
    Suppression effective des doublons.
    Trust=propose ‚Üí user DOIT valider via Telegram.
    """
    deleted_files = await delete_duplicates_from_csv(csv_path)

    return ActionResult(
        input_summary=f"Suppression doublons ({len(deleted_files)} fichiers)",
        output_summary=f"‚Üí {sum(f.size_mb for f in deleted_files):.1f} Go lib√©r√©s",
        confidence=1.0,
        reasoning=f"SHA256 match + r√®gles priorit√© appliqu√©es",
        payload={
            "deleted_files": [f.path for f in deleted_files],
            "freed_gb": sum(f.size_mb for f in deleted_files) / 1024,
        }
    )
```

---

## üß™ Tests

### Test 1 - Scan avec exclusions syst√®me
```python
@pytest.mark.asyncio
async def test_scan_excludes_system_folders():
    scan = PCScan(root=r"C:\Users\lopez\")
    files = await scan.collect_files()

    # V√©rifier aucun fichier dans AppData, .cache, etc.
    for f in files:
        assert "AppData" not in f.path
        assert ".cache" not in f.path
```

### Test 2 - D√©tection doublons SHA256
```python
@pytest.mark.asyncio
async def test_sha256_detects_duplicates():
    # Cr√©er 3 fichiers identiques (contenu) dans dossiers diff√©rents
    files = [
        create_temp_file("BeeStation/Photos/test.jpg", content=PHOTO_BYTES),
        create_temp_file("OneDrive/test.jpg", content=PHOTO_BYTES),
        create_temp_file("Downloads/test (2).jpg", content=PHOTO_BYTES),
    ]

    scan = PCScan(root=temp_dir)
    duplicates = await scan.find_duplicates()

    assert len(duplicates) == 1  # 1 groupe de 3 fichiers
    assert len(duplicates[0].files) == 3
```

### Test 3 - R√®gles priorit√© emplacement
```python
@pytest.mark.asyncio
async def test_priority_keeps_beestation_photos():
    group = DuplicateGroup(sha256="abc123", files=[
        FileHash(path=r"C:\Users\lopez\BeeStation\Photos\test.jpg", size_bytes=1000),
        FileHash(path=r"C:\Users\lopez\OneDrive\test.jpg", size_bytes=1000),
        FileHash(path=r"C:\Users\lopez\Downloads\test.jpg", size_bytes=1000),
    ])

    to_keep = select_file_to_keep(group.files)

    assert to_keep.path == r"C:\Users\lopez\BeeStation\Photos\test.jpg"
```

### Test 4 - R√®gles priorit√© r√©solution
```python
@pytest.mark.asyncio
async def test_priority_keeps_highest_resolution():
    # M√™me emplacement (non-prioritaire), r√©solutions diff√©rentes
    group = DuplicateGroup(sha256="abc123", files=[
        FileHash(path=r"C:\Users\lopez\OneDrive\test1.jpg", resolution="1920x1080"),
        FileHash(path=r"C:\Users\lopez\OneDrive\test2.jpg", resolution="4032x3024"),  # Meilleure
        FileHash(path=r"C:\Users\lopez\OneDrive\test3.jpg", resolution="1920x1080"),
    ])

    to_keep = select_file_to_keep(group.files)

    assert to_keep.path == r"C:\Users\lopez\OneDrive\test2.jpg"
    assert to_keep.resolution == "4032x3024"
```

### Test 5 - Dry-run ne supprime rien
```python
@pytest.mark.asyncio
async def test_dryrun_does_not_delete():
    files_before = count_files(temp_dir)

    report = await scan_and_generate_csv(temp_dir, dry_run=True)

    files_after = count_files(temp_dir)
    assert files_before == files_after  # Aucune suppression
    assert os.path.exists(report.csv_path)  # CSV cr√©√©
```

### Test 6 - Rapport CSV complet
```python
@pytest.mark.asyncio
async def test_csv_report_format():
    report = await generate_dedup_report()

    df = pd.read_csv(report.csv_path)

    # V√©rifier colonnes obligatoires
    assert set(df.columns) == {"sha256", "action", "file_path", "size_mb", "resolution", "exif_date", "reason"}

    # V√©rifier 1 seul KEEP par groupe SHA256
    for sha, group in df.groupby("sha256"):
        assert group[group["action"] == "KEEP"].shape[0] == 1
```

---

## üìä M√©triques de succ√®s

- **Performance** : Scan 100 Go en <30 min (SSD)
- **Pr√©cision** : 100% fiabilit√© SHA256 (0 faux positifs)
- **UX** : Dry-run CSV valid√© avant toute suppression
- **Trust** : Scan auto, suppression propose (validation obligatoire)

---

## üîó D√©pendances techniques

- **SHA256** : `hashlib` (Python stdlib)
- **EXIF** : `pillow` (d√©j√† install√© Story 3.5)
- **SQLite** : Cache local SHA256 pour scan incr√©mental
- **CSV** : `pandas` pour g√©n√©ration rapport
- **Telegram** : Envoi document CSV + inline buttons validation

---

## üìù Notes impl√©mentation

### Scope Story 3.8
- **IN SCOPE** : D√©duplication uniquement (scan + suppression doublons)
- **OUT OF SCOPE** : Classification/organisation automatique des documents (user le fera manuellement apr√®s scan)

### Chemins prioritaires
- **Photos** : `C:\Users\lopez\BeeStation\Photos\` (priorit√© absolue)
- **Documents** : `C:\Users\lopez\BeeStation\Documents\` (priorit√© absolue)
- User d√©placera manuellement les documents vers Archives apr√®s d√©duplication

### Scan incr√©mental (optionnel Story 3.8.1 future)
- SQLite cache permet r√©scan partiel (v√©rifier mtime avant recalcul SHA256)
- Commande `/rescan-pc` pour forcer full rescan

### Gestion erreurs
- Fichier verrouill√© (en cours d'utilisation) ‚Üí skip + log warning
- Permission denied ‚Üí skip + log warning
- Fichier supprim√© entre scan et ex√©cution ‚Üí skip + log info

---

## üöÄ D√©ploiement

1. **Agent archiviste** : `agents/src/agents/archiviste/dedup_scanner.py`
2. **Commande Telegram** : `bot/handlers/commands.py` (ajout `/scan-photos-pc`)
3. **Cache SQLite** : Cr√©er `C:\Friday\scan-cache\` (mkdir auto si absent)
4. **Logs** : `C:\Friday\logs\dedup-*.log` (rotation 30 jours)

---

## üéØ Impact utilisateur

**Avant** : 100+ Go photos cumul√©es 25 ans, triples/quadruples manuels impossibles √† g√©rer
**Apr√®s** : Scan automatique, rapport CSV clair, suppression intelligente valid√©e par user

**Gain estim√©** : 30-50 Go lib√©r√©s (30-50% doublons typiques sur 25 ans de photos)
