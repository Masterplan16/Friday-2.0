# Politique utilisation mod√®les IA - Friday 2.0

**Date** : 2026-02-05
**Version** : 1.0.0

---

## Vue d'ensemble

Friday 2.0 utilise des mod√®les IA externes (Mistral, Gemini, Claude) et locaux (Ollama).
Cette politique d√©finit les r√®gles de versionnage, upgrade, et monitoring des mod√®les.

---

## R√®gles de versionnage

### Environnements

| Environnement | Strat√©gie | Justification |
|--------------|-----------|---------------|
| **Dev/Test** | Suffixe `-latest` | Tester nouveaux mod√®les en continu |
| **Staging** | Version explicite | Valider performance avant prod |
| **Production** | Version explicite | Stabilit√© et reproductibilit√© |

### Exemples

**Mistral** :
- Dev : `mistral-large-latest` (suit automatiquement les releases)
- Staging : `mistral-large-2411` (fixer version candidate)
- Production : `mistral-large-2411` (apr√®s validation accuracy)

**Gemini** :
- Dev : `gemini-2.0-flash-latest`
- Staging : `gemini-2.0-flash-001`
- Production : `gemini-2.0-flash-001`

**Claude** :
- Dev : `claude-3-5-sonnet-latest`
- Staging : `claude-3-5-sonnet-20241022`
- Production : `claude-3-5-sonnet-20241022`

**Ollama (local)** :
- Dev : `nemotron:12b-instruct` (pas de suffixe -latest pour Ollama)
- Staging : `nemotron:12b-instruct`
- Production : `nemotron:12b-instruct`

---

## Proc√©dure d'upgrade

### Phase 1 : Test en dev

1. **Activer `-latest` en dev**
   ```python
   # agents/src/config/settings.py
   LLM_MODEL = os.getenv("LLM_MODEL", "mistral-large-latest")  # Dev uniquement
   ```

2. **Tester pendant 1 semaine**
   - Ex√©cuter tests unitaires + int√©gration
   - Valider accuracy sur datasets de r√©f√©rence (tests/fixtures/)
   - Surveiller m√©triques :
     ```python
     {
         "llm.accuracy.email_classification": 0.95,
         "llm.latency.p99_ms": 1200,
         "llm.cost.per_1k_tokens": 0.03
     }
     ```

3. **Identifier nouvelle version stable**
   ```bash
   # Exemple : -latest pointe maintenant vers mistral-large-2501
   curl https://api.mistral.ai/v1/models | jq '.data[] | select(.id | contains("large"))'
   # Output : "id": "mistral-large-2501"
   ```

### Phase 2 : Validation staging

4. **D√©ployer version explicite en staging**
   ```python
   # agents/src/config/settings.py (staging)
   LLM_MODEL = os.getenv("LLM_MODEL", "mistral-large-2501")  # Version candidate
   ```

5. **Tests approfondis (2 semaines)**
   - Rejouer 100+ emails r√©els (archive tests/fixtures/email_classification.json)
   - Comparer accuracy avec version actuelle production
   - Crit√®res validation :
     - Accuracy >= version actuelle (pas de r√©gression)
     - Latency p99 <= +20% max
     - Cost <= +30% max (sauf si accuracy +10%)

6. **D√©cision Go/No-Go**
   - Go : Accuracy maintenue OU am√©lior√©e
   - No-Go : R√©gression >3% ‚Üí Rester sur version actuelle

### Phase 3 : D√©ploiement production

7. **Mise √† jour progressive**
   ```bash
   # 1. Backup config actuelle
   cp .env.prod .env.prod.bak

   # 2. Update LLM_MODEL
   sed -i 's/mistral-large-2411/mistral-large-2501/g' .env.prod

   # 3. Red√©marrer agents (rolling restart)
   docker compose up -d --no-deps agents
   ```

8. **Monitoring renforc√© (72h)**
   - Alertes sur accuracy <90% (seuil normal : <85%)
   - Alertes sur latency p99 >2000ms
   - Surveillance corrections manuelles Antonio (feedback loop)

9. **Rollback si probl√®me**
   ```bash
   # Restaurer version pr√©c√©dente
   cp .env.prod.bak .env.prod
   docker compose up -d --no-deps agents

   # Documenter dans Decision Log
   echo "Rollback mistral-large-2501 ‚Üí 2411 : accuracy drop 92% ‚Üí 88%" >> docs/DECISION_LOG.md
   ```

---

## Surveillance continue

### M√©triques par mod√®le

**Stockage PostgreSQL** :
```sql
-- Table : core.llm_metrics
CREATE TABLE core.llm_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    model_id TEXT NOT NULL,                -- "mistral-large-2411"
    module TEXT NOT NULL,                  -- "email", "archiviste", etc.
    action TEXT NOT NULL,                  -- "classify", "summarize", etc.
    accuracy DECIMAL(5,4),                 -- 0.9523 (calcul√© depuis corrections)
    latency_p50_ms INT,
    latency_p95_ms INT,
    latency_p99_ms INT,
    cost_per_1k_tokens DECIMAL(8,6),
    window_start TIMESTAMPTZ NOT NULL,    -- Fen√™tre hebdomadaire
    window_end TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_llm_metrics_model ON core.llm_metrics(model_id, module, action, window_start);
```

**Calcul nightly** :
```python
# services/metrics/nightly.py
async def compute_llm_metrics():
    """Agr√®ge m√©triques LLM par mod√®le/module/action sur fen√™tre glissante 7j"""
    for model_id in ["mistral-large-2411", "mistral-small-latest"]:
        for module in ["email", "archiviste", "financial"]:
            accuracy = await calculate_accuracy(model_id, module, days=7)
            latency = await calculate_latency_percentiles(model_id, module, days=7)
            cost = await calculate_cost(model_id, module, days=7)

            await db.execute("""
                INSERT INTO core.llm_metrics
                (model_id, module, action, accuracy, latency_p99_ms, cost_per_1k_tokens, window_start, window_end)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            """, model_id, module, "all", accuracy, latency["p99"], cost, start, end)
```

### Dashboard Telegram `/confiance`

```
üìä Confiance mod√®les IA (7 derniers jours)

ü§ñ mistral-large-2411
  ‚Ä¢ Email classification : 95.2% (‚úÖ stable)
  ‚Ä¢ Email summarize : 93.8% (‚ö†Ô∏è -1.2% vs semaine derni√®re)
  ‚Ä¢ Latency p99 : 1150ms
  ‚Ä¢ Cost : 0.028‚Ç¨/1k tokens

ü§ñ mistral-small-latest
  ‚Ä¢ Archiviste categorize : 88.5% (‚ùå <90%, r√©trograd√© √† propose)
  ‚Ä¢ Latency p99 : 480ms
  ‚Ä¢ Cost : 0.012‚Ç¨/1k tokens

üìà Tendances
  ‚Ä¢ Accuracy globale : 93.1% (-0.5% vs semaine derni√®re)
  ‚Ä¢ Total corrections Antonio : 12 cette semaine
```

---

## Gestion des co√ªts

### Budgets mensuels

| Mod√®le | Usage | Budget max/mois | Alertes |
|--------|-------|-----------------|---------|
| Mistral Large | Classification emails, r√©sum√©s | 20‚Ç¨ | Si >15‚Ç¨ |
| Mistral Small | Embeddings, queries simples | 5‚Ç¨ | Si >4‚Ç¨ |
| Gemini Flash | OCR post-processing | 10‚Ç¨ | Si >8‚Ç¨ |
| Ollama local | Donn√©es ultra-sensibles (m√©dical) | 0‚Ç¨ (√©lectricit√© VPS) | - |

### Optimisations

**R√®gles automatiques** :
- Si co√ªt >budget ‚Üí Basculer sur mod√®le moins cher (Large ‚Üí Small)
- Si accuracy baisse <85% apr√®s bascule ‚Üí Revenir mod√®le cher + alerte Antonio

**Strat√©gies manuelles** :
- Batch processing (traiter 10 emails ‚Üí 1 appel LLM)
- Cache aggressive (r√©sum√©s identiques)
- Ollama local pour use cases tol√©rants latence (+500ms)

---

## Matrix de d√©cision mod√®le

### Quand utiliser Mistral Large ?

| Crit√®re | Seuil |
|---------|-------|
| Complexit√© t√¢che | Classification multi-label (>10 classes) |
| Accuracy requise | >95% |
| Donn√©es sensibles | Non (sinon Ollama local) |
| Budget disponible | >50% budget mensuel restant |

**Exemples** : Email classification (urgent/important), Financial categorization

### Quand utiliser Mistral Small ?

| Crit√®re | Seuil |
|---------|-------|
| Complexit√© t√¢che | Classification binaire/simple |
| Accuracy acceptable | >90% |
| Volume √©lev√© | >100 requ√™tes/jour |
| Budget serr√© | <30% budget mensuel restant |

**Exemples** : Spam detection, Simple summaries, Embeddings

### Quand utiliser Ollama local ?

| Crit√®re | Seuil |
|---------|-------|
| Donn√©es sensibles | RGPD strict (m√©dical, financier, juridique) |
| Latency tol√©rable | >2 secondes OK |
| Accuracy acceptable | >85% |
| Z√©ro co√ªt API | Requis |

**Exemples** : Analyse dossier m√©dical, Extraction donn√©es bancaires, Contrats juridiques

---

## Anti-patterns (INTERDITS)

### 1. Hardcoder model IDs sans env var

```python
# ‚ùå INCORRECT
response = mistral.chat(model="mistral-large-2411", messages=...)

# ‚úÖ CORRECT
response = mistral.chat(model=settings.LLM_MODEL, messages=...)
```

### 2. Utiliser `-latest` en production

```python
# ‚ùå INCORRECT (prod)
LLM_MODEL = "mistral-large-latest"  # Version non d√©terministe

# ‚úÖ CORRECT (prod)
LLM_MODEL = "mistral-large-2411"  # Version fixe
```

### 3. Ignorer accuracy drops

```python
# ‚ùå INCORRECT
if accuracy < 0.80:
    logger.warning("Accuracy faible")  # Pas d'action

# ‚úÖ CORRECT
if accuracy < 0.85:
    await downgrade_trust_level(module, action)
    await alert_telegram(f"‚ö†Ô∏è Accuracy {module}.{action} : {accuracy:.1%}")
```

---

## R√©f√©rences

### Documentation API

- **Mistral AI** : https://docs.mistral.ai/api/
- **Gemini** : https://ai.google.dev/gemini-api/docs
- **Claude** : https://docs.anthropic.com/claude/reference
- **Ollama** : https://ollama.com/library

### Model cards

| Mod√®le | Context window | Output max | Prix (input/output) |
|--------|---------------|------------|---------------------|
| mistral-large-2411 | 128k tokens | 4k tokens | 0.002‚Ç¨ / 0.006‚Ç¨ per 1k tokens |
| mistral-small-2412 | 32k tokens | 8k tokens | 0.0002‚Ç¨ / 0.0006‚Ç¨ per 1k tokens |
| gemini-2.0-flash-001 | 1M tokens | 8k tokens | 0.00001‚Ç¨ / 0.00003‚Ç¨ per 1k tokens |
| nemotron:12b-instruct | Illimit√© (local) | Illimit√© | 0‚Ç¨ (√©lectricit√© VPS) |

---

## Changelog

| Date | Change | Raison |
|------|--------|--------|
| 2026-02-05 | Cr√©ation document | Code review adversarial v2 finding #25 |
| 2026-02-05 | Ajout matrix d√©cision mod√®le | Clarifier r√®gles usage Large vs Small vs Ollama |

---

**Version** : 1.0.0
**Prochaine r√©vision** : Apr√®s Story 2 (Email Agent) en production
