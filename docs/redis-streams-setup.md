# Redis Streams - Setup & Configuration

**Date** : 2026-02-05
**Version** : 1.0.0
**Objectif** : Configuration compl√®te Redis Streams pour √©v√©nements critiques Friday 2.0

---

## üéØ Principe

Friday 2.0 utilise **Redis Streams** (pas Pub/Sub) pour les √©v√©nements critiques afin de garantir :
- ‚úÖ Delivery garanti (m√™me si consumer temporairement down)
- ‚úÖ Persistence des √©v√©nements
- ‚úÖ Replay possible en cas d'erreur
- ‚úÖ Consumer groups pour load balancing

**√âv√©nements critiques** (via Redis Streams) :
- `email.received`
- `document.processed`
- `pipeline.error`
- `service.down`
- `trust.level.changed`
- `action.corrected`
- `action.validated`

**√âv√©nements informatifs** (via Redis Pub/Sub) :
- `agent.completed`
- `file.uploaded`

---

## üìö Concepts Redis Streams

### **Stream = Log d'√©v√©nements**

Chaque √©v√©nement a un **ID unique** (timestamp + s√©quence) :
```
1517574547834-0  ‚Üí timestamp-sequence
```

### **Consumer Group = Groupe de workers**

Plusieurs consumers peuvent lire le m√™me stream en parall√®le sans dupliquer le travail.

### **Pending Entries List (PEL)**

√âv√©nements "en cours de traitement" par un consumer. Permet retry si consumer crash.

---

## üõ†Ô∏è Setup initial

### **1. Cr√©er les streams et consumer groups**

```bash
# Script: scripts/setup-redis-streams.sh

#!/bin/bash

REDIS_HOST=${REDIS_HOST:-localhost}
REDIS_PORT=${REDIS_PORT:-6379}
REDIS_PASSWORD=${REDIS_PASSWORD:-}

# Helper function
create_stream_group() {
    local stream=$1
    local group=$2

    echo "üìù Creating consumer group: $stream ‚Üí $group"

    if [ -n "$REDIS_PASSWORD" ]; then
        redis-cli -h $REDIS_HOST -p $REDIS_PORT -a "$REDIS_PASSWORD" --no-auth-warning \
            XGROUP CREATE $stream $group $ MKSTREAM
    else
        redis-cli -h $REDIS_HOST -p $REDIS_PORT \
            XGROUP CREATE $stream $group $ MKSTREAM
    fi
}

echo "üöÄ Setup Redis Streams for Friday 2.0"
echo "========================================"

# Cr√©er consumer groups pour chaque stream critique
create_stream_group "email.received" "email-processor"
create_stream_group "document.processed" "document-indexer"
create_stream_group "pipeline.error" "error-handler"
create_stream_group "service.down" "monitoring"
create_stream_group "trust.level.changed" "trust-manager"
create_stream_group "action.corrected" "feedback-loop"
create_stream_group "action.validated" "trust-manager"

echo ""
echo "‚úÖ Redis Streams setup complete!"
echo ""
echo "Verify with:"
echo "  redis-cli XINFO GROUPS email.received"
```

**Ex√©cuter** :
```bash
chmod +x scripts/setup-redis-streams.sh
./scripts/setup-redis-streams.sh
```

### **2. V√©rifier la cr√©ation**

```bash
# Lister les groups d'un stream
redis-cli XINFO GROUPS email.received

# Output attendu:
# 1) name: email-processor
# 2) consumers: 0
# 3) pending: 0
# 4) last-delivered-id: 0-0
```

---

## üì§ Producer : Publier un √©v√©nement

### **Python (asyncio)**

```python
# agents/src/utils/redis_streams.py

import redis.asyncio as redis
import json
from typing import Dict, Any

class RedisStreamsPublisher:
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url, decode_responses=True)

    async def publish_event(self, stream: str, payload: Dict[str, Any]) -> str:
        """
        Publie un √©v√©nement dans un stream Redis

        Returns:
            event_id: ID de l'√©v√©nement publi√© (ex: "1517574547834-0")
        """
        # S√©rialiser le payload en JSON string
        serialized_payload = {
            k: json.dumps(v) if isinstance(v, (dict, list)) else str(v)
            for k, v in payload.items()
        }

        # XADD: Ajouter √©v√©nement au stream
        event_id = await self.redis.xadd(
            stream,
            serialized_payload,
            maxlen=10000,  # Garder max 10k √©v√©nements (FIFO)
            approximate=True  # Performance: trim approximatif OK
        )

        return event_id

# Usage
publisher = RedisStreamsPublisher("redis://localhost:6379")

await publisher.publish_event("email.received", {
    "email_id": "abc123",
    "category": "medical",
    "priority": "high",
    "has_attachments": True
})
```

### **n8n (HTTP Request node)**

```javascript
// n8n: Publish to Redis Stream node

const stream = "email.received";
const payload = {
    email_id: $json.email_id,
    category: $json.category,
    priority: $json.priority,
    has_attachments: $json.attachments.length > 0
};

// Appeler FastAPI endpoint qui publish dans Redis
return {
    method: "POST",
    url: `http://gateway:8000/api/v1/events/publish`,
    body: {
        stream: stream,
        payload: payload
    }
};
```

---

## üì• Consumer : Lire les √©v√©nements

### **Python (asyncio) - Consumer simple**

```python
# services/email-processor/consumer.py

import redis.asyncio as redis
import json
import asyncio

class RedisStreamsConsumer:
    def __init__(self, redis_url: str, stream: str, group: str, consumer_name: str):
        self.redis = redis.from_url(redis_url, decode_responses=True)
        self.stream = stream
        self.group = group
        self.consumer_name = consumer_name

    async def consume(self, handler):
        """
        Consomme les √©v√©nements d'un stream avec consumer group

        Args:
            handler: Fonction async √† appeler pour chaque √©v√©nement
        """
        print(f"üîÑ Listening to {self.stream} as {self.group}/{self.consumer_name}")

        while True:
            try:
                # XREADGROUP: Lire nouveaux √©v√©nements
                events = await self.redis.xreadgroup(
                    groupname=self.group,
                    consumername=self.consumer_name,
                    streams={self.stream: ">"},  # ">" = nouveaux messages uniquement
                    count=10,  # Batch de 10 √©v√©nements max
                    block=5000  # Block 5s si aucun √©v√©nement
                )

                if not events:
                    continue  # Timeout, retry

                for stream_name, messages in events:
                    for event_id, payload in messages:
                        try:
                            # D√©s√©rialiser payload
                            data = {
                                k: json.loads(v) if v.startswith(("{", "[")) else v
                                for k, v in payload.items()
                            }

                            # Traiter √©v√©nement
                            await handler(event_id, data)

                            # ACK: Marquer comme trait√©
                            await self.redis.xack(self.stream, self.group, event_id)

                        except Exception as e:
                            print(f"‚ùå Error processing {event_id}: {e}")
                            # Ne pas ACK ‚Üí restera dans Pending List

            except asyncio.CancelledError:
                print("üõë Consumer stopped")
                break
            except Exception as e:
                print(f"‚ùå Consumer error: {e}")
                await asyncio.sleep(5)  # Retry apr√®s 5s

# Usage
async def handle_email_received(event_id: str, payload: dict):
    print(f"üìß Processing email {payload['email_id']}")
    # ... traitement ...

consumer = RedisStreamsConsumer(
    redis_url="redis://localhost:6379",
    stream="email.received",
    group="email-processor",
    consumer_name="worker-1"
)

await consumer.consume(handle_email_received)
```

---

## üîÑ Retry & Recovery

### **Pending List : R√©cup√©rer √©v√©nements non ACK√©s**

Si un consumer crash avant d'ACK, les √©v√©nements restent dans la **Pending Entries List**.

**Script de recovery** :

```python
async def claim_pending_events(self, idle_time_ms: int = 60000):
    """
    R√©cup√®re les √©v√©nements pending depuis plus de idle_time_ms

    Args:
        idle_time_ms: Temps minimum depuis dernier delivery (d√©faut: 60s)
    """
    # XPENDING: Lister √©v√©nements pending
    pending = await self.redis.xpending_range(
        self.stream,
        self.group,
        min="-",
        max="+",
        count=100
    )

    if not pending:
        return

    print(f"‚ö†Ô∏è  Found {len(pending)} pending events")

    for entry in pending:
        event_id = entry['message_id']
        consumer = entry['consumer']
        idle_ms = entry['time_since_delivered']

        if idle_ms < idle_time_ms:
            continue  # Pas encore timeout

        # XCLAIM: R√©clamer l'√©v√©nement
        claimed = await self.redis.xclaim(
            self.stream,
            self.group,
            self.consumer_name,
            min_idle_time=idle_time_ms,
            message_ids=[event_id]
        )

        if claimed:
            event_id_claimed, payload = claimed[0]
            print(f"üîÅ Reclaimed event {event_id} from {consumer}")
            # Retraiter l'√©v√©nement...
```

**Cron de recovery** (toutes les minutes) :

```python
# services/recovery/cron.py

async def recovery_loop():
    consumer = RedisStreamsConsumer(...)

    while True:
        await consumer.claim_pending_events(idle_time_ms=60000)
        await asyncio.sleep(60)  # V√©rifier toutes les 1min
```

---

## üîç Monitoring

### **Dashboard Redis Streams**

```bash
# scripts/redis-streams-status.sh

#!/bin/bash

echo "üìä Redis Streams Status"
echo "======================="

for stream in "email.received" "document.processed" "pipeline.error"; do
    echo ""
    echo "Stream: $stream"
    echo "----------------------------------------"

    # Longueur du stream
    redis-cli XLEN $stream

    # Consumer groups
    redis-cli XINFO GROUPS $stream

    # Pending entries
    redis-cli XPENDING $stream email-processor
done
```

### **M√©triques √† surveiller**

| M√©trique | Commande | Alerte si |
|----------|----------|-----------|
| Stream length | `XLEN email.received` | > 1000 (backlog) |
| Pending count | `XPENDING email.received email-processor` | > 100 (consumers lents) |
| Lag (derniers 5min) | Custom script | > 500 events |
| Consumer actifs | `XINFO GROUPS` | = 0 (aucun consumer) |

---

## üßπ Maintenance

### **Trim automatique**

Redis Streams garde tous les √©v√©nements par d√©faut. Utiliser `MAXLEN` pour limiter :

```bash
# Garder max 10k √©v√©nements par stream
redis-cli XTRIM email.received MAXLEN ~ 10000
```

### **Supprimer anciens consumer groups**

```bash
# Supprimer un consumer group (si plus utilis√©)
redis-cli XGROUP DESTROY email.received old-processor
```

---

## üìã Checklist production

- [ ] Consumer groups cr√©√©s (`./scripts/setup-redis-streams.sh`)
- [ ] Consumers d√©marr√©s (1+ par groupe)
- [ ] Recovery cron actif (claim pending events)
- [ ] Monitoring alertes configur√©es (backlog, pending, lag)
- [ ] MAXLEN configur√© sur tous les streams (limite taille)
- [ ] Tests end-to-end passent (publish ‚Üí consume ‚Üí ACK)

---

**Cr√©√© le** : 2026-02-05
**Version** : 1.0.0
**Contributeur** : Claude (Code Review Adversarial - Issue #4)
