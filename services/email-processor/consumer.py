# services/email-processor/consumer.py
# Consumer Redis Streams pour √©v√©nements email.received
# Partie de Story 2 : Moteur Vie (Email Pipeline)

import asyncio
import json
import logging
from typing import Any, Dict

import redis.asyncio as redis
from agents.src.middleware.trust import ActionResult, friday_action

logger = logging.getLogger(__name__)


class EmailProcessorConsumer:
    """
    Consumer Redis Streams pour traiter les √©v√©nements email.received

    Workflow:
    1. √âcoute stream "email.received" via consumer group "email-processor"
    2. Pour chaque √©v√©nement, d√©clenche les actions downstream:
       - Extraction t√¢ches (si email contient TODOs)
       - Extraction √©v√©nements agenda (si email contient dates/invitations)
       - Envoi notification Telegram si priorit√© HIGH
    3. ACK l'√©v√©nement apr√®s traitement r√©ussi
    """

    def __init__(
        self,
        redis_url: str,
        stream: str = "email.received",
        group: str = "email-processor",
        consumer_name: str = "worker-1",
    ):
        self.redis = redis.from_url(redis_url, decode_responses=True)
        self.stream = stream
        self.group = group
        self.consumer_name = consumer_name

    async def start(self):
        """D√©marre le consumer (boucle infinie)"""
        logger.info(f"üîÑ Starting Email Processor Consumer: {self.group}/{self.consumer_name}")

        while True:
            try:
                # XREADGROUP : Lire nouveaux √©v√©nements du stream
                events = await self.redis.xreadgroup(
                    groupname=self.group,
                    consumername=self.consumer_name,
                    streams={self.stream: ">"},  # ">" = nouveaux messages uniquement
                    count=10,  # Batch de 10 √©v√©nements max
                    block=5000,  # Block 5s si aucun √©v√©nement
                )

                if not events:
                    continue  # Timeout, retry

                for stream_name, messages in events:
                    for event_id, payload in messages:
                        try:
                            # D√©s√©rialiser payload
                            data = self._deserialize_payload(payload)

                            # Traiter √©v√©nement
                            await self.process_email_received(event_id, data)

                            # ACK: Marquer comme trait√©
                            await self.redis.xack(self.stream, self.group, event_id)

                        except Exception as e:
                            logger.error(f"‚ùå Error processing {event_id}: {e}", exc_info=True)
                            # Ne pas ACK ‚Üí restera dans Pending List pour retry

            except asyncio.CancelledError:
                logger.info("üõë Consumer stopped")
                break
            except Exception as e:
                logger.error(f"‚ùå Consumer error: {e}", exc_info=True)
                await asyncio.sleep(5)  # Retry apr√®s 5s

    def _deserialize_payload(self, payload: Dict[str, str]) -> Dict[str, Any]:
        """D√©s√©rialise le payload Redis (JSON strings)"""
        return {k: json.loads(v) if v.startswith(("{", "[")) else v for k, v in payload.items()}

    async def process_email_received(self, event_id: str, data: Dict[str, Any]):
        """
        Traite un √©v√©nement email.received

        Args:
            event_id: ID de l'√©v√©nement Redis (ex: "1517574547834-0")
            data: Payload d√©s√©rialis√© {email_id, category, priority, has_attachments}
        """
        email_id = data.get("email_id")
        category = data.get("category")
        priority = data.get("priority")
        has_attachments = data.get("has_attachments", False)

        logger.info(f"üìß Processing email {email_id} (category={category}, priority={priority})")

        # 1. Si priorit√© HIGH ‚Üí Notification Telegram imm√©diate
        if priority == "high":
            await self._send_telegram_notification(email_id, category)

        # 2. Extraction t√¢ches (si cat√©gorie pertinente)
        if category in ["professional", "thesis", "personal"]:
            await self._extract_tasks_from_email(email_id)

        # 3. Extraction √©v√©nements agenda (si email contient dates)
        await self._extract_events_from_email(email_id)

        # 4. Si attachments ‚Üí Trigger document processing
        if has_attachments:
            await self._trigger_attachment_processing(email_id)

        logger.info(f"‚úÖ Email {email_id} processed successfully")

    async def _send_telegram_notification(self, email_id: str, category: str):
        """Envoie notification Telegram pour email prioritaire"""
        # TODO Story 2: Impl√©menter envoi Telegram via bot API
        logger.info(
            f"üì± [TODO] Send Telegram notification for email {email_id} (category={category})"
        )

    async def _extract_tasks_from_email(self, email_id: str):
        """Extrait t√¢ches d√©tect√©es dans l'email"""
        # TODO Story 2: Impl√©menter extraction t√¢ches via agent email
        logger.info(f"üìã [TODO] Extract tasks from email {email_id}")

    async def _extract_events_from_email(self, email_id: str):
        """Extrait √©v√©nements agenda d√©tect√©s dans l'email"""
        # TODO Story 9: Impl√©menter extraction √©v√©nements via agent agenda
        logger.info(f"üìÖ [TODO] Extract events from email {email_id}")

    async def _trigger_attachment_processing(self, email_id: str):
        """D√©clenche traitement des pi√®ces jointes"""
        # TODO Story 3: Publier √©v√©nement Redis pour archiviste
        logger.info(f"üìé [TODO] Trigger attachment processing for email {email_id}")

    async def claim_pending_events(self, idle_time_ms: int = 60000):
        """
        R√©cup√®re les √©v√©nements pending depuis plus de idle_time_ms (recovery)

        Args:
            idle_time_ms: Temps minimum depuis dernier delivery (d√©faut: 60s)
        """
        pending = await self.redis.xpending_range(
            self.stream, self.group, min="-", max="+", count=100
        )

        if not pending:
            return

        logger.warning(f"‚ö†Ô∏è  Found {len(pending)} pending events, attempting recovery...")

        for entry in pending:
            event_id = entry["message_id"]
            consumer = entry["consumer"]
            idle_ms = entry["time_since_delivered"]

            if idle_ms < idle_time_ms:
                continue  # Pas encore timeout

            # XCLAIM: R√©clamer l'√©v√©nement
            claimed = await self.redis.xclaim(
                self.stream,
                self.group,
                self.consumer_name,
                min_idle_time=idle_time_ms,
                message_ids=[event_id],
            )

            if claimed:
                event_id_claimed, payload = claimed[0]
                logger.info(f"üîÅ Reclaimed event {event_id} from {consumer}")

                # Retraiter l'√©v√©nement
                data = self._deserialize_payload(payload)
                await self.process_email_received(event_id_claimed, data)
                await self.redis.xack(self.stream, self.group, event_id_claimed)


async def main():
    """Point d'entr√©e du consumer (pour Docker Compose service)"""
    import os

    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    consumer_name = os.getenv("CONSUMER_NAME", f"worker-{os.getpid()}")

    consumer = EmailProcessorConsumer(redis_url=redis_url, consumer_name=consumer_name)

    # Lancer recovery des pending events toutes les minutes
    async def recovery_loop():
        while True:
            await asyncio.sleep(60)
            await consumer.claim_pending_events()

    # Lancer consumer + recovery en parall√®le
    await asyncio.gather(consumer.start(), recovery_loop())


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    asyncio.run(main())
